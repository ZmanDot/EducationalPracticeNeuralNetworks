{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Процесс обучения нейросетей\n",
        "\n",
        "Обучение нейросетей — это ключевой этап, на котором сеть «учится» решать задачи, анализируя данные и подстраивая свои внутренние параметры. Основная цель процесса обучения — найти такие значения весов (параметров), при которых сеть будет с максимальной точностью предсказывать правильные ответы. Обучение нейросети можно сравнить с тем, как человек учится новой деятельности: сначала выполняются простые шаги, а с практикой решения становятся точнее и сложнее.\n",
        "\n",
        "![](https://neiros.ru/img/neural-networks/metody-obucheniya-neyrosetey.png)\n",
        "- Супервизорное обучение(с учителем) — нейросеть обучается на данных, для которых известны правильные ответы. Например, классификация изображений, где каждому изображению соответствует определённая метка.\n",
        "\n",
        "- Без учителя — сеть пытается найти закономерности в данных без заранее известных меток, например, при кластеризации данных.\n",
        "\n",
        "- Обучение с подкреплением — нейросеть взаимодействует с окружающей средой и получает положительное или отрицательное подкрепление за свои действия. Подходит для задач, где необходимо оптимизировать последовательность решений, например, игры или управление роботами.\n",
        "\n",
        "### Основные этапы процесса обучения\n",
        "\n",
        "1. **Инициализация весов**  \n",
        "   При запуске нейросети её веса устанавливаются случайным образом или с небольшим начальным значением. Этот этап называется инициализацией, и он влияет на скорость и эффективность обучения. В процессе обучения веса будут изменяться и оптимизироваться для минимизации ошибки предсказания.\n",
        "\n",
        "2. **Формирование данных для обучения**  \n",
        "   Данные разделяются на обучающую и тестовую выборки. Обучающая выборка — это данные, на которых сеть будет «учиться», а тестовая — данные, которые используются для проверки точности работы сети. Иногда данные также разделяются на дополнительный набор для валидации, что помогает настроить параметры сети до её окончательной проверки на тестовой выборке.\n",
        "\n",
        "3. **Прямое распространение (Forward Propagation)**  \n",
        "   В процессе прямого распространения входные данные проходят через слои нейронов, начиная от входного слоя до выходного. На каждом слое нейроны преобразуют входные сигналы в новый сигнал, который передается дальше по сети. На основе полученного ответа на выходе сети можно оценить, насколько предсказание близко к правильному значению.\n",
        "\n",
        "4. **Вычисление ошибки**  \n",
        "   Ошибка — это разница между предсказанным сетью значением и реальным значением из обучающих данных. Для её расчета используют функции ошибки (например, среднеквадратичную ошибку для регрессии или кросс-энтропию для классификации). Ошибка показывает, насколько сеть отклоняется от ожидаемого результата, и указывает, что веса нужно корректировать.\n",
        "\n",
        "5. **Обратное распространение ошибки (Backpropagation)**  \n",
        "   После вычисления ошибки начинается процесс обратного распространения ошибки. Это этап, на котором рассчитываются градиенты весов и смещений для каждого нейрона, чтобы определить, в какую сторону нужно изменить значения параметров сети. Градиентный спуск помогает корректировать веса, двигаясь в сторону минимизации функции ошибки. Весь процесс обратного распространения повторяется на каждом слое, пока сеть не найдёт оптимальные веса для всех соединений.\n",
        "\n",
        "6. **Градиентный спуск и оптимизация**  \n",
        "   Градиентный спуск — это алгоритм, используемый для нахождения минимальных значений функции ошибки, т.е. для корректировки весов в сторону уменьшения ошибки. Он проходит через «шаги» по направлению уменьшения ошибки, корректируя параметры сети. Существуют различные варианты градиентного спуска: стохастический (SGD), мини-батч градиентный спуск и адаптивные методы (например, Adam), каждый из которых влияет на скорость и точность обучения.\n",
        "\n",
        "![](https://i0.wp.com/apptractor.ru/wp-content/uploads/2024/05/credit-analytics-vidya.jpg?w=1200&ssl=1)\n",
        "\n",
        "- Стандартный градиентный спуск (Batch Gradient Descent): Вычисляет градиент, используя все обучающие данные на каждом шаге, что может быть неэффективно для больших наборов данных.\n",
        "- Стохастический градиентный спуск (Stochastic Gradient Descent, SGD): Вычисляет градиент на основе одного обучающего примера, выбранного случайным образом, на каждом шаге, что делает алгоритм более быстрым и способным к обновлению модели онлайн.\n",
        "- Мини-пакетный градиентный спуск (Mini-batch Gradient Descent): Компромисс между двумя предыдущими методами, вычисляет градиент на основе небольшой выборки данных (мини-пакета), что позволяет более эффективно использовать ресурсы и обеспечивает более стабильное схождение.\n",
        "\n",
        "\n",
        "### Train and Test\n",
        "\n",
        "1. **Train(Валидация)**  \n",
        "   Во время обучения нейросеть периодически проверяется на тренировочном наборе данных, чтобы следить за тем, что её эффективность на этих данных не ухудшается. Если ошибка на тренировочном наборе начинает возрастать, это может означать переобучение — когда сеть слишком точно запоминает обучающие данные, но плохо обобщает результаты на новые примеры.\n",
        "\n",
        "2. **Test(Тестирование)**  \n",
        "   После завершения обучения сеть тестируется на тестовом наборе данных, который ранее не использовался для обучения. Это помогает проверить, насколько хорошо сеть справляется с новой информацией, и оценить её точность и надёжность.\n",
        "\n",
        "### Заключительные этапы\n",
        "\n",
        "1. **Тонкая настройка**  \n",
        "   После основной фазы обучения сеть может быть донастроена, изменяя параметры, такие как количество слоёв, функции активации или алгоритмы оптимизации, чтобы добиться ещё более высокой точности.\n",
        "\n",
        "2. **Выход на продакшн**  \n",
        "   Когда сеть показывает достаточную точность и надёжность, она может быть использована в реальных условиях, будь то обработка изображений, текстов или других данных. Важно понимать, что даже после внедрения сеть может продолжать дообучаться на новых данных, чтобы улучшать свои результаты."
      ],
      "metadata": {
        "id": "2nGb_nPAZ_Jd"
      }
    }
  ]
}